{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Memory Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End-to-End Memory network built with the memory_model.py script is designed to recieve a story and a question (query) as inputs and return a yes/no answer to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load In Model\n",
    "\n",
    "The first step is to load in the model structure and weights. The model is stored as a json object so we will use the model_from_json method from keras.models to load it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mason\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\mason\\AppData\\Local\\Continuum\\anaconda3\\envs\\nlp_course\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "# load json and create model\n",
    "with open('model.json', 'r') as f:\n",
    "    model = model_from_json(f.read())\n",
    "    \n",
    "# load weights into model\n",
    "model.load_weights(\"chatbot_150epochs.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Vocabulary, Tokenizer, and Supporting Files\n",
    "\n",
    "Next, we need to load the vocabulary that the model was trained on, as well as the tokenizer object that is used to vectorize the words in the stories, questions, and answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pickle\n",
    "with open('vocabulary.txt','r') as f:\n",
    "   vocab = ast.literal_eval(f.read())\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "with open('parameters.txt','r') as f:\n",
    "   params = ast.literal_eval(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'went', 'kitchen', 'discarded', 'milk', 'picked', 'left', 'Is', 'got', 'office', 'hallway', 'there', 'bedroom', 'travelled', 'moved', 'to', 'garden', 'down', 'Daniel', 'the', 'up', 'bathroom', '.', 'no', 'took', 'grabbed', 'journeyed', '?', 'dropped', 'in', 'put', 'John', 'back', 'football', 'Sandra', 'Mary', 'yes', 'apple'}\n"
     ]
    }
   ],
   "source": [
    "# Words in the Vocabulary\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Custom Story/Question\n",
    "\n",
    "We now have all the tools we need to test our model on a custom story and question. Keep in mind, we can only use words from the vocabulary that the model was trained on. (We also have to be mindful of the extra space between the last word in a sentence and the ending punctuation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define story variables\n",
    "my_story = \"Sandra travelled up to the bedroom . Daniel went in the office . Mary grabbed the milk .\"\n",
    "my_question = \"Is Daniel in the office ?\"\n",
    "my_answer = 'yes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use the .split() method on our variables in order to store them in the same format as the training data. We then pass our data through our custom vectorize_stories function that we have defined in the network_functions.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = [(my_story.split(),my_question.split(),my_answer)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_functions import vectorize_stories\n",
    "my_story,my_ques,my_ans = vectorize_stories(mydata,\n",
    "                                            word_index=tokenizer.word_index,\n",
    "                                            max_story_len=params['max_story_len'],\n",
    "                                            max_question_len=params['max_question_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = model.predict(([ my_story, my_ques]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer is:  yes\n",
      "Probability of certainty was:  88.13 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate prediction from model\n",
    "def prediction(pred_results):\n",
    "    val_max = np.argmax(pred_results[0])\n",
    "    for key, val in tokenizer.word_index.items():\n",
    "        if val == val_max:\n",
    "            k = key\n",
    "    print(\"Predicted answer is: \", k)\n",
    "    print(\"Probability of certainty was: \", str(round(100*pred_results[0][val_max],2)),\"%\")\n",
    "    return {'prediction':k,'probability':pred_results[0][val_max]}\n",
    "\n",
    "pred_ansnwer = prediction(pred_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
